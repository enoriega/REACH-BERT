test:
  seed_everything: 1987
  trainer:
    precision: bf16 #bfloat16 available in Ampere GPUs (my 3060)
    # Accumulate gradiaents based on the batch size in the training data
#    accumulate_grad_batches: 40 # RoBERTa's batch size is 8k, so work out this val based on the batch size
    logger: true
    callbacks:
      # Configure checkpointing behavior here
      - class_path: pytorch_lightning.callbacks.ModelCheckpoint
        init_args:
          dirpath: ckpts
          filename: 'reach_bert-epoch_{epoch}-step_{step}-val_loss_{Val Loss:.3f}'
          save_top_k: 3
          auto_insert_metric_name: false
          monitor: "Val Loss"
    gpus: 1
    enable_progress_bar: true
    fast_dev_run: false
  model:
    backbone_model_name: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
  data:
    # Directory with the data files w/o masking
    data_dir: /media/evo870/data/reach-bert-data/bert_files_w_negatives
    # Directory with the multiple versions of masked data files
    masked_data_dir: /media/evo870/data/reach-bert-data/masked_bert_files_w_negatives
    tokenizer_model_name: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
    num_workers: 8
    # Best batch size for an RTX 3060
    batch_size: 3000
    # This is a hyper param tuned using data_processing/sequence_length_stats.py
    max_seq_len: 69
    # Set to true to regrenerate the dataset index every run
    overwrite_dataset_index: false
#  ckpt_path: ckpts/reach_bert-epoch_9-step_141941-val_loss_0.619.ckpt
