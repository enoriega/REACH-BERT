fit:
  seed_everything: 1987
  trainer:
    logger: true
    callbacks:
      # Configure checkpointing behavior here
      - class_path: pytorch_lightning.callbacks.ModelCheckpoint
        init_args:
          dirpath: ckpts
          save_top_k: 3
          auto_insert_metric_name: false
          monitor: "Loss/Combined Val"
      # Custom callback to shift the pre-masked version of the dataset each train epoch (see RoBERTa)
      - class_path: data_loaders.callbacks.ShiftMasksCallback
    gpus: 1
    enable_progress_bar: true
    fast_dev_run: true
    # Run validation every 10% of an epoch
    val_check_interval: 0.1
  model:
    backbone_model_name: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
  data:
    # Directory with the data files w/o masking
    data_dir: /media/evo870/data/reach-bert-data/bert_files_w_negatives
    # Directory with the multiple versions of masked data files
    masked_data_dir: /media/evo870/data/reach-bert-data/masked_bert_files_w_negatives
    tokenizer_model_name: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
    num_workers: 8
    # Best batch size for an RTX 3060
    batch_size: 80
    # This is a hyper param tuned using data_processing/sequence_length_stats.py
    max_seq_len: 69
    # Set to true to regrenerate the dataset index every run
    overwrite_dataset_index: false
#  ckpt_path: ckpts
